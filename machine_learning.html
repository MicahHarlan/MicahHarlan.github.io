<html>
	<head>
		<title>Machine Learning</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

<br> 
<br>
<section>
	<div id="rows"> 
		<article class="center
		">
			<h1><strong> Logistic Regression: Predicting a Country</strong></h1>
				<p> The task of my classifier was to predict the country a Tsunami occurred in. My classifier was terrible. The classifier has a cross validation score of 0.120. This classifier was doomed from the start, there are 98 countries to choose from. None of the features in my classifier are essentially good. The ones I used are, deaths, houses destroyed, and the height of each wave. Each variable performs equally as well when running them alone.
					When evaluating the <u> ADDLINK TO confusion matrix</u>A it was too big to read. I calculated the f1 score which was .002. Which told me my classifier is not better than a random guess. The recall score was 0.10 and the precision was even worse at .003. Both recall and precision need to be improved.
					I started off my PCA by throwing random features from my data set into it To compare improvement I also ran a 2 fold cross validation (any higher number for k takes 20+ minutes run). The score for my linear model with PCA was .1809 and the cross validation score was .178. Just adding these features in have increased the score. The next thing i did was removing some of these features I added to see which ones are useful, or donâ€™t help at all. Below is how each feature effected the classifier when it was absent from the list of features. The whole feature list that was added is, normalized total deaths, normalized total destroyed houses, normalized wave height, city names, normalized year, normalized validity of the tsunami.
				</p>
				<br> 
				<list> 
					<li> Removing normalized total deaths PCA score: 0.1814%.</li>
					<li>Removing normalized total destroyed houses PCA score: 0.1809%.</li>
					<li>Removing normalized wave height PCA score: 0.1785%.</li>
					<li>Removing city names PCA score: 0.1751%.</li>
					<li>Removing normalized PCA normalized year: .1820%.</li>
					<li>Removing normalized PCA normalized validity: 0.1536%.</li>
				</list>
				<br> 
				<p>From these results I removed the features that increased or had no impact on the score when absent. These features were: total houses destroyed and year, the score is now 0.1815%, which as good only removing the normalized year. 
					I decided to put normalized houses back into my feature list bringing it back up to 0.1820%. In the previous runs The number of principle components was 5 (except when the I removed to features I dropped it to 4). 
					The next thing I did to improve the score was running a gridsearch to find the best number of principle components.
					This gridsearch took a while but it returned the value 4 for the best number of principle components, leaving it at the same score as before.
				</p>
				<h2><strong> Final Features Chosen </strong></h2>
				<list>
					<li>Normalized deaths</li>
					<li>Normalized height of waves</li>
					<li>City Names</li>
					<li>Normalized validity</li>
					<li>Normalized number of houses destroyed</li>
				</list>
				<h2>Final Score: 0.1820</h2>
				<br> 
				<h1>Support Vector Machine</h1>
				<p>For my support vector machine I changed the target variable to make a Tsunami in the USA detector to make my 
					target binary, instead of it consisting of 98 target choices.
					 The first run of my program was an SVM with a PCA inside of it using the LinearSVC from sklearn gave a cross validation score of 0.871. 
					 This was really hard to improve from just messing around with the features. 
					 I went through and removed each feature but, kept the remaining features, each 
					 feature removed gave me a score of 0.871. The list of features is normalized deaths,
					  normalized height of waves, city names, normalized validity, and normalized number of houses destroyed. 
					 I next decided to add features, I chose to add latitude and longitude. When I added latitude and longitude 
					 and it went up to 92% accurate I decided to not keep it. Even though it made my classifier really good, 
					I would like to challenge myself. Predicting location based off of location is not too impressive and 
					to me it feels like the easy way out.</p>
					<br>
					<p>
						The next thing I did was finding which SVM kernel is the best one to use, 
						which returned polynomial for the kernel, this returned the same score as before. In that same gridsearch I also tried to 
						find the best C value from a list containing 0.1, 1, 10, 100, and 1,000. 
						This returned 0.1. The next gridsearch I did returned 1 for the best value of PCA components
					</p>
					<br>
					<h2>Final Features Chosen</h2>
					<list>
						<li>Nomalized deaths</li>
						<li>Normalized height</li>
						<li>Normalized Destoryed Houses</li>
					</list>
					<br>
					<h2>Final Score: 0.871</h2>
					<p>
					My SVM classifier had little improvement after doing various grid searches and messing around with the features. 
					Both my linear model and SVM perform the same. 0.87% accuracy is not too bad of a classifier. 
					From both my SVM and logistic regression I learned that PCA is a very useful tool to improve scores.</p>
		</article>

		<article class="center"><h1>The code Embeded with the Data File Will be here soon</h1></article>
	</div>
</section>



</body>
</html>